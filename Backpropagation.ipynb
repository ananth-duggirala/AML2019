{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backpropagation Algorithm\n",
    "#This is part of Assignment 5 for the AML course.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['Type1','Type2','Type3']].values\n",
    "\n",
    "# Inputs: we define x and y here.\n",
    "X = df.drop(['Type1','Type2','Type3'], axis = 1)\n",
    "X.shape, y.shape # Print shapes \n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(input_matrix, model):\n",
    "    #loading model parameters\n",
    "    weight1 = model['weight1'] #random initialized weights\n",
    "    bias1 = model['bias1'] #bias\n",
    "    \n",
    "    weight2 = model['weight2'] #random initialized weights\n",
    "    bias2 = model['bias2'] #bias\n",
    "        \n",
    "    weight3 = model['weight3'] #random initialized weights\n",
    "    bias3 = model['bias3'] #bias\n",
    "    \n",
    "    # first linear step forward\n",
    "    r1 = input_matrix.dot(weight1) + bias1\n",
    "    \n",
    "    # first activation function\n",
    "    M1 = np.tanh(r1)\n",
    "    \n",
    "    # second linear step forward and second activation function\n",
    "    r2 = M1.dot(weight2) + bias2\n",
    "    M2 = np.tanh(r2)\n",
    "        \n",
    "    # third linear step\n",
    "    r3 = M2.dot(weight3) + bias3\n",
    "\n",
    "    # We now use the softmax function\n",
    "    M3 = softmax(r3)\n",
    "    \n",
    "    # return these values\n",
    "    cache = {'M0': input_matrix,'r1':r1,'M1':M1,'r2':r2,'M2':M2,'r3':r3,'M3':M3}\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation function\n",
    "\n",
    "def backward_propagation(model,cache,y):\n",
    "    \n",
    "    # load parameters from the model, and forward propagation results\n",
    "    weight1 = model['weight1']\n",
    "    weight2 = model['weight2']\n",
    "    weight3 = model['weight3']\n",
    "    \n",
    "    bias1 = model['bias1']\n",
    "    bias2 = model['bias2']\n",
    "    bias3 = model['bias3']\n",
    "    \n",
    "    M0 = cache['M0']\n",
    "    M1 = cache['M1']\n",
    "    M2 = cache['M2']\n",
    "    M3 = cache['M3']\n",
    "\n",
    "    # Get number of samples\n",
    "    n_samples = y.shape[0] \n",
    "    \n",
    "    \n",
    "    # Calculate loss derivatives wrt weights, biases, and layers \n",
    "    d_z3 = M3 - y #loss derivative\n",
    "    d_weight3 = 1/n_samples*(M2.T).dot(d_z3)\n",
    "    d_bias3 = 1/n_samples*np.sum(d_z3, axis=0)\n",
    "    \n",
    "    d_z2 = np.multiply(d_z3.dot(weight3.T), 1 - np.power(M2,2)) #tanh derivative\n",
    "    d_weight2 = 1/n_samples*np.dot(M1.T, d_z2)\n",
    "    d_bias2 = 1/n_samples*np.sum(d_z2, axis=0)\n",
    "    \n",
    "    d_z1 = np.multiply(d_z2.dot(weight2.T), 1 - np.power(M1,2)) #tanh derivative\n",
    "    d_weight1 = 1/n_samples*np.dot(M0.T,d_z1)\n",
    "    d_bias1 = 1/n_samples*np.sum(d_z1,axis=0)\n",
    "    \n",
    "    # Return gradients\n",
    "    gradients = {'weight3':d_weight3, 'bias3':d_bias3, 'weight2':d_weight2,'bias2':d_bias2,'weight1':d_weight1,'bias1':d_bias1}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to compute softmax and softmax loss function\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    \n",
    "def softmax_loss(y,y_hat):\n",
    "    clipping_value = 0.00000000001\n",
    "    n_samples = y.shape[0] # number of samples    \n",
    "    \n",
    "    # Loss formula\n",
    "    # The complete matrix is summed up by np.sum. Therefore we do not need to implement both sums in the formula separately.\n",
    "    loss = -1/ np.sum(y * np.log(y_hat.clip(min = clipping_value))) * n_samples\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to initialize parameters\n",
    "def initialize_params(input_dim, hidden_dim, output_dim):\n",
    "    # first layer weights and bias\n",
    "    weight1 = np.random.randn(input_dim, hidden_dim) * 2 - 1\n",
    "    bias1 = np.zeros((1, hidden_dim))\n",
    "\n",
    "    # second layer weights and bias\n",
    "    weight2 = np.random.randn(hidden_dim, hidden_dim) * 2 - 1\n",
    "    bias2 = np.zeros((1, hidden_dim))\n",
    "    \n",
    "    # third layer weights and bias\n",
    "    weight3  = np.random.randn(hidden_dim, output_dim) * 2 - 1\n",
    "    bias3 = np.zeros((1, output_dim))\n",
    "    \n",
    "    model = {'weight1': weight1, 'weight2': weight2, 'weight3': weight3, 'bias1': bias1, 'bias2': bias2, 'bias3': bias3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to update parameters\n",
    "def update_params(model, rate, gradients):\n",
    "    # load weights and biases from the model\n",
    "    weight1 = model['weight1']\n",
    "    weight2 = model['weight2']\n",
    "    weight3 = model['weight3']\n",
    "    \n",
    "    bias1 = model['bias1']\n",
    "    bias2 = model['bias2']\n",
    "    bias3 = model['bias3']\n",
    "    \n",
    "    # update them\n",
    "    weight1 = weight1 - rate * gradients['weight1']\n",
    "    weight2 = weight2 - rate * gradients['weight2']\n",
    "    weight3 = weight3 - rate * gradients['weight3']\n",
    "    bias1 = bias1 - rate * gradients['bias1'] \n",
    "    bias2 = bias2 - rate * gradients['bias2']\n",
    "    bias3 = bias3 - rate * gradients['bias3']\n",
    "    \n",
    "    # return updated parameters\n",
    "    model = {'weight1': weight1, 'weight2': weight2, 'weight3': weight3, 'bias1': bias1, 'bias2': bias2, 'bias3': bias3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    fwd = forward_propagation(model, x)\n",
    "    return np.argmax(fwd['M3'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, x, y):\n",
    "    n_samples = y.shape[0]\n",
    "    predicted = predict(model, x)\n",
    "    predicted = prediction.reshape(y.shape) # reshape so that prediction and y have the same shape\n",
    "    \n",
    "    np.sum(np.abs(predicted - y)) # number of wrong examples\n",
    "    accuracy = (m - error) * 100.0 /m\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "losses = [] # empty list\n",
    "def train_model(model, x1, y1, rate, epochs = 2000, print_loss = False):\n",
    "    for i in range(epochs):\n",
    "        cache = forward_propagation(x1, model)\n",
    "        M1 = cache['M1']\n",
    "        M2 = cache['M2']\n",
    "        gradients = backward_propagation(model, cache, y1)\n",
    "        model = update_params(model, rate, gradients)\n",
    "\n",
    "        # print \n",
    "        if print_loss and i % 100 == 0:\n",
    "            M3 = cache['M3']\n",
    "            print(\"Loss after iteration \", i, \" is \", softmax_loss(y1,M3))\n",
    "            y_hat = predict(x1, model)\n",
    "            y_true = y1.argmax(axis=1)\n",
    "            print(\"Accuracy after iteration \",i,' is ',accuracy_score(y_hat, y_true)*100,\"%\")\n",
    "            losses.append(accuracy_score(y_hat, y_true) * 100)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration  0  is  0.27188015634576246\n",
      "Accuracy after iteration  0  is  23.595505617977526 %\n",
      "Loss after iteration  100  is  3.1817467875056633\n",
      "Accuracy after iteration  100  is  87.64044943820225 %\n",
      "Loss after iteration  200  is  4.443480846199877\n",
      "Accuracy after iteration  200  is  89.8876404494382 %\n",
      "Loss after iteration  300  is  4.948245172964192\n",
      "Accuracy after iteration  300  is  91.01123595505618 %\n",
      "Loss after iteration  400  is  5.24334113677698\n",
      "Accuracy after iteration  400  is  92.69662921348315 %\n",
      "Loss after iteration  500  is  5.524385244083084\n",
      "Accuracy after iteration  500  is  93.25842696629213 %\n",
      "Loss after iteration  600  is  5.741075643312949\n",
      "Accuracy after iteration  600  is  93.82022471910112 %\n",
      "Loss after iteration  700  is  6.152363640710933\n",
      "Accuracy after iteration  700  is  93.82022471910112 %\n",
      "Loss after iteration  800  is  7.540288360632298\n",
      "Accuracy after iteration  800  is  94.9438202247191 %\n",
      "Loss after iteration  900  is  7.889019405224605\n",
      "Accuracy after iteration  900  is  94.9438202247191 %\n",
      "Loss after iteration  1000  is  8.149441542057815\n",
      "Accuracy after iteration  1000  is  94.9438202247191 %\n",
      "Loss after iteration  1100  is  8.425556794528156\n",
      "Accuracy after iteration  1100  is  94.9438202247191 %\n",
      "Loss after iteration  1200  is  8.78105271100521\n",
      "Accuracy after iteration  1200  is  94.9438202247191 %\n",
      "Loss after iteration  1300  is  9.221874404197177\n",
      "Accuracy after iteration  1300  is  95.50561797752809 %\n",
      "Loss after iteration  1400  is  9.740356269568137\n",
      "Accuracy after iteration  1400  is  96.06741573033707 %\n",
      "Loss after iteration  1500  is  10.437773200868337\n",
      "Accuracy after iteration  1500  is  96.06741573033707 %\n",
      "Loss after iteration  1600  is  11.295512193620628\n",
      "Accuracy after iteration  1600  is  96.62921348314607 %\n",
      "Loss after iteration  1700  is  12.023776424163586\n",
      "Accuracy after iteration  1700  is  96.62921348314607 %\n",
      "Loss after iteration  1800  is  12.522051300959331\n",
      "Accuracy after iteration  1800  is  96.62921348314607 %\n",
      "Loss after iteration  1900  is  12.932790695915834\n",
      "Accuracy after iteration  1900  is  96.62921348314607 %\n",
      "Loss after iteration  2000  is  13.326910690200748\n",
      "Accuracy after iteration  2000  is  96.62921348314607 %\n",
      "Loss after iteration  2100  is  13.791331707701922\n",
      "Accuracy after iteration  2100  is  96.62921348314607 %\n",
      "Loss after iteration  2200  is  14.429858869192469\n",
      "Accuracy after iteration  2200  is  96.62921348314607 %\n",
      "Loss after iteration  2300  is  15.092605728896178\n",
      "Accuracy after iteration  2300  is  96.62921348314607 %\n",
      "Loss after iteration  2400  is  15.532215855035368\n",
      "Accuracy after iteration  2400  is  96.62921348314607 %\n",
      "Loss after iteration  2500  is  15.867365564661696\n",
      "Accuracy after iteration  2500  is  97.19101123595506 %\n",
      "Loss after iteration  2600  is  16.260763377121986\n",
      "Accuracy after iteration  2600  is  97.19101123595506 %\n",
      "Loss after iteration  2700  is  16.85247534422333\n",
      "Accuracy after iteration  2700  is  97.19101123595506 %\n",
      "Loss after iteration  2800  is  17.573865636005003\n",
      "Accuracy after iteration  2800  is  97.19101123595506 %\n",
      "Loss after iteration  2900  is  18.479960243273908\n",
      "Accuracy after iteration  2900  is  97.75280898876404 %\n",
      "Loss after iteration  3000  is  19.87177960882653\n",
      "Accuracy after iteration  3000  is  97.75280898876404 %\n",
      "Loss after iteration  3100  is  20.467724196694803\n",
      "Accuracy after iteration  3100  is  97.75280898876404 %\n",
      "Loss after iteration  3200  is  20.78817051354128\n",
      "Accuracy after iteration  3200  is  97.75280898876404 %\n",
      "Loss after iteration  3300  is  21.026406220591937\n",
      "Accuracy after iteration  3300  is  97.75280898876404 %\n",
      "Loss after iteration  3400  is  21.230589744555587\n",
      "Accuracy after iteration  3400  is  97.75280898876404 %\n",
      "Loss after iteration  3500  is  21.416182198243796\n",
      "Accuracy after iteration  3500  is  97.75280898876404 %\n",
      "Loss after iteration  3600  is  21.5892158617498\n",
      "Accuracy after iteration  3600  is  97.75280898876404 %\n",
      "Loss after iteration  3700  is  21.752373570606796\n",
      "Accuracy after iteration  3700  is  97.75280898876404 %\n",
      "Loss after iteration  3800  is  21.906981444804604\n",
      "Accuracy after iteration  3800  is  97.75280898876404 %\n",
      "Loss after iteration  3900  is  22.053793359263533\n",
      "Accuracy after iteration  3900  is  97.75280898876404 %\n",
      "Loss after iteration  4000  is  22.193332146096733\n",
      "Accuracy after iteration  4000  is  97.75280898876404 %\n",
      "Loss after iteration  4100  is  22.326037780524867\n",
      "Accuracy after iteration  4100  is  97.75280898876404 %\n",
      "Loss after iteration  4200  is  22.45232482928902\n",
      "Accuracy after iteration  4200  is  97.75280898876404 %\n",
      "Loss after iteration  4300  is  22.572598725628442\n",
      "Accuracy after iteration  4300  is  97.75280898876404 %\n",
      "Loss after iteration  4400  is  22.687255992305893\n",
      "Accuracy after iteration  4400  is  97.75280898876404 %\n",
      "Loss after iteration  4500  is  22.796680222583863\n",
      "Accuracy after iteration  4500  is  97.75280898876404 %\n",
      "Loss after iteration  4600  is  22.90123840361757\n",
      "Accuracy after iteration  4600  is  97.75280898876404 %\n",
      "Loss after iteration  4700  is  23.00127865168792\n",
      "Accuracy after iteration  4700  is  97.75280898876404 %\n",
      "Loss after iteration  4800  is  23.097129025354214\n",
      "Accuracy after iteration  4800  is  97.75280898876404 %\n",
      "Loss after iteration  4900  is  23.189096726881864\n",
      "Accuracy after iteration  4900  is  97.75280898876404 %\n",
      "Loss after iteration  5000  is  23.277467078884982\n",
      "Accuracy after iteration  5000  is  97.75280898876404 %\n",
      "Loss after iteration  5100  is  23.362501874750038\n",
      "Accuracy after iteration  5100  is  97.75280898876404 %\n",
      "Loss after iteration  5200  is  23.44443694006961\n",
      "Accuracy after iteration  5200  is  97.75280898876404 %\n",
      "Loss after iteration  5300  is  23.523478994504842\n",
      "Accuracy after iteration  5300  is  97.75280898876404 %\n",
      "Loss after iteration  5400  is  23.599802187081014\n",
      "Accuracy after iteration  5400  is  97.75280898876404 %\n",
      "Loss after iteration  5500  is  23.67354499651137\n",
      "Accuracy after iteration  5500  is  97.75280898876404 %\n",
      "Loss after iteration  5600  is  23.74480849414856\n",
      "Accuracy after iteration  5600  is  97.75280898876404 %\n",
      "Loss after iteration  5700  is  23.8136571406055\n",
      "Accuracy after iteration  5700  is  97.75280898876404 %\n",
      "Loss after iteration  5800  is  23.880123154811155\n",
      "Accuracy after iteration  5800  is  97.75280898876404 %\n",
      "Loss after iteration  5900  is  23.944214915315047\n",
      "Accuracy after iteration  5900  is  97.75280898876404 %\n",
      "Loss after iteration  6000  is  24.005928856489756\n",
      "Accuracy after iteration  6000  is  97.75280898876404 %\n",
      "Loss after iteration  6100  is  24.065263203109758\n",
      "Accuracy after iteration  6100  is  97.75280898876404 %\n",
      "Loss after iteration  6200  is  24.122231139179583\n",
      "Accuracy after iteration  6200  is  97.75280898876404 %\n",
      "Loss after iteration  6300  is  24.176871050244536\n",
      "Accuracy after iteration  6300  is  97.75280898876404 %\n",
      "Loss after iteration  6400  is  24.229252357538293\n",
      "Accuracy after iteration  6400  is  97.75280898876404 %\n",
      "Loss after iteration  6500  is  24.279476783814456\n",
      "Accuracy after iteration  6500  is  97.75280898876404 %\n",
      "Loss after iteration  6600  is  24.327676077380545\n",
      "Accuracy after iteration  6600  is  97.75280898876404 %\n",
      "Loss after iteration  6700  is  24.37400787046863\n",
      "Accuracy after iteration  6700  is  97.75280898876404 %\n",
      "Loss after iteration  6800  is  24.41865141135666\n",
      "Accuracy after iteration  6800  is  97.75280898876404 %\n",
      "Loss after iteration  6900  is  24.461804610263997\n",
      "Accuracy after iteration  6900  is  97.75280898876404 %\n",
      "Loss after iteration  7000  is  24.50368347891496\n",
      "Accuracy after iteration  7000  is  97.75280898876404 %\n",
      "Loss after iteration  7100  is  24.544524864831473\n",
      "Accuracy after iteration  7100  is  97.75280898876404 %\n",
      "Loss after iteration  7200  is  24.584593547654\n",
      "Accuracy after iteration  7200  is  97.75280898876404 %\n",
      "Loss after iteration  7300  is  24.624195438267662\n",
      "Accuracy after iteration  7300  is  97.75280898876404 %\n",
      "Loss after iteration  7400  is  24.663700109267815\n",
      "Accuracy after iteration  7400  is  97.75280898876404 %\n",
      "Loss after iteration  7500  is  24.703578905071556\n",
      "Accuracy after iteration  7500  is  97.75280898876404 %\n",
      "Loss after iteration  7600  is  24.744471164017376\n",
      "Accuracy after iteration  7600  is  97.75280898876404 %\n",
      "Loss after iteration  7700  is  24.787304997371912\n",
      "Accuracy after iteration  7700  is  97.75280898876404 %\n",
      "Loss after iteration  7800  is  24.83353244522154\n",
      "Accuracy after iteration  7800  is  97.75280898876404 %\n",
      "Loss after iteration  7900  is  24.885627104456564\n",
      "Accuracy after iteration  7900  is  97.75280898876404 %\n",
      "Loss after iteration  8000  is  24.948255847252863\n",
      "Accuracy after iteration  8000  is  97.75280898876404 %\n",
      "Loss after iteration  8100  is  25.03145434521639\n",
      "Accuracy after iteration  8100  is  98.31460674157303 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration  8200  is  25.16104684970279\n",
      "Accuracy after iteration  8200  is  98.31460674157303 %\n",
      "Loss after iteration  8300  is  25.422800306895937\n",
      "Accuracy after iteration  8300  is  98.31460674157303 %\n",
      "Loss after iteration  8400  is  26.17002481851801\n",
      "Accuracy after iteration  8400  is  98.31460674157303 %\n",
      "Loss after iteration  8500  is  27.397995556991823\n",
      "Accuracy after iteration  8500  is  98.31460674157303 %\n",
      "Loss after iteration  8600  is  27.772552448818075\n",
      "Accuracy after iteration  8600  is  98.31460674157303 %\n",
      "Loss after iteration  8700  is  27.89394490741048\n",
      "Accuracy after iteration  8700  is  98.31460674157303 %\n",
      "Loss after iteration  8800  is  27.966987603894932\n",
      "Accuracy after iteration  8800  is  98.31460674157303 %\n",
      "Loss after iteration  8900  is  28.025428578513083\n",
      "Accuracy after iteration  8900  is  98.31460674157303 %\n"
     ]
    }
   ],
   "source": [
    "model = initialize_params(13, 5, 3)\n",
    "model = train_model(model, X, y, rate = 0.07, epochs=9000, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d3e4553208>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
